{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2a696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas to open data files & processing it.\n",
    "import pandas as pd\n",
    "# to see all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "# To see whole text\n",
    "pd.set_option('max_colwidth', -1)\n",
    "\n",
    "# numpy for numeric data processing\n",
    "import numpy as np\n",
    "\n",
    "# keras for deep learning model creation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# to fix random seeds\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Regular Expression for text cleaning\n",
    "import re\n",
    "\n",
    "# to track the progress - progress bar\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f506e48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('file:///Users/DeLaLuna/Downloads/train-balanced-sarcasm.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc921be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['author', 'subreddit', 'score', 'ups', 'downs', 'date', 'created_utc', 'parent_comment'], axis=1, inplace=True)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc26212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    505405\n",
      "1    505368\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_counts = df['label'].value_counts()\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3755f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1010773 entries, 0 to 1010825\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count    Dtype \n",
      "---  ------   --------------    ----- \n",
      " 0   label    1010773 non-null  int64 \n",
      " 1   comment  1010773 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 23.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3a2460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mispell_dict = {\"ain't\": \"is not\", \"cannot\": \"can not\", \"aren't\": \"are not\", \"can't\": \"can not\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",\n",
    "                \"doesn't\": \"does not\",\n",
    "                \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
    "                \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\",\n",
    "                \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n",
    "                \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
    "                \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\", \"this's\": \"this is\", \"that'd\": \"that would\",\n",
    "                \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\", \"they'd\": \"they would\",\n",
    "                \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
    "                \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "                \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\",\n",
    "                \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"wont\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
    "                \"wouldn't\": \"would not\",\n",
    "                \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n",
    "                \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color',\n",
    "                'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n",
    "                'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What',\n",
    "                'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I',\n",
    "                'theBest': 'the best', 'howdoes': 'how does', 'Etherium': 'Ethereum',\n",
    "                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what',\n",
    "                'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "\n",
    "mispell_dict = {k.lower(): v.lower() for k, v in mispell_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07726dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(s):\n",
    "    # making our string lowercase & removing extra spaces\n",
    "    s = str(s).lower().strip()\n",
    "    \n",
    "    # remove contractions.\n",
    "    s = \" \".join([mispell_dict[word] if word in mispell_dict.keys() else word for word in s.split()])\n",
    "    \n",
    "    # removing \\n\n",
    "    s = re.sub('\\n', '', s)\n",
    "    \n",
    "    # put spaces before & after punctuations to make words seprate. Like \"king?\" to \"king\", \"?\".\n",
    "    s = re.sub(r\"([?!,+=—&%\\'\\\";:¿।।।|\\(\\){}\\[\\]//])\", r\" \\1 \", s)\n",
    "    \n",
    "    # Remove more than 2 continues spaces with 1 space.\n",
    "    s = re.sub('[ ]{2,}', ' ', s).strip()\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbcacde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment'] = df['comment'].apply(preprocessing_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f56e8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 271808\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Concatenate all text into a single string\n",
    "text_combined = df['comment'].str.cat(sep=' ')\n",
    "\n",
    "# Step 2: Tokenize the combined text string into individual words\n",
    "words_seperated = text_combined.split()\n",
    "\n",
    "# Step 3: Count the number of unique words\n",
    "unique_word_count = len(set(words_seperated))\n",
    "\n",
    "print(\"Number of unique words:\", unique_word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52012d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total unique words we are going to use.\n",
    "TOTAL_WORDS = 271808\n",
    "\n",
    "# max number of words one sentence can have\n",
    "MAX_LEN = 50\n",
    "\n",
    "# width of of 1D embedding vector\n",
    "EMBEDDING_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6861953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.4 s, sys: 493 ms, total: 40.9 s\n",
      "Wall time: 41.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer(num_words=TOTAL_WORDS)\n",
    "tokenizer.fit_on_texts(list(df['comment']))\n",
    "\n",
    "train_data = tokenizer.texts_to_sequences(df['comment'])\n",
    "train_data = pad_sequences(train_data, maxlen = MAX_LEN)\n",
    "target = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8db27d1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/DeLaLuna/Downloads/fasttext-crawl-300d-2m/crawl-300d-2M.vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:6\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/DeLaLuna/Downloads/fasttext-crawl-300d-2m/crawl-300d-2M.vec'"
     ]
    }
   ],
   "source": [
    "#########%%time\n",
    "###EMBEDDING_FILE = '/Users/DeLaLuna/Downloads/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "\n",
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in tqdm(open(EMBEDDING_FILE)))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(TOTAL_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "688c05fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78abc5f1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/DeLaLuna/Downloads/fasttext-crawl-300d-2m/crawl-300d-2M.vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m word, np\u001b[38;5;241m.\u001b[39masarray(arr, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the embedding file and create the embeddings_index dictionary\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mEMBEDDING_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      9\u001b[0m     embeddings_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(get_coefs(\u001b[38;5;241m*\u001b[39mo\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m tqdm(file))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Get the word index from the tokenizer\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/DeLaLuna/Downloads/fasttext-crawl-300d-2m/crawl-300d-2M.vec'"
     ]
    }
   ],
   "source": [
    "##########EMBEDDING_FILE = '/Users/DeLaLuna/Downloads/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "\n",
    "# Define the function to get word coefficients from the embedding file\n",
    "###def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "# Load the embedding file and create the embeddings_index dictionary\n",
    "with open(EMBEDDING_FILE, 'r') as file:\n",
    "    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in tqdm(file))\n",
    "\n",
    "# Get the word index from the tokenizer\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Determine the number of words and size of the embedding matrix\n",
    "nb_words = min(TOTAL_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798257b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "########import io\n",
    "\n",
    "####def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5449eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading unique words: 100%|█| 1010773/1010773 [00:02<00:00, 385426.82 comments/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 271808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "unique_words = set()\n",
    "\n",
    "# Iterate over each comment and split into words\n",
    "for comment in tqdm(df['comment'], desc='Loading unique words', unit=' comments'):\n",
    "    words = comment.split()\n",
    "    unique_words.update(words)\n",
    "\n",
    "# Display the total number of unique words\n",
    "print(\"Total unique words:\", len(unique_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6e60e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fullest.',\n",
       " 'crashed..',\n",
       " 'freetime.',\n",
       " 'hotfixing',\n",
       " 'xeons.',\n",
       " 'loooolll',\n",
       " 'weefee',\n",
       " 'moderate-carb',\n",
       " 'non-creationists',\n",
       " 'blinding',\n",
       " 'no-contact',\n",
       " '*sexist.',\n",
       " 'money...hmm...',\n",
       " 'felwort',\n",
       " 'inproving',\n",
       " 'tasered',\n",
       " 'maaaagic',\n",
       " 'marco_rubio',\n",
       " 'pen**.',\n",
       " 'bun',\n",
       " 'byak',\n",
       " 'f14-t',\n",
       " 'oled',\n",
       " 'sh*t',\n",
       " 'cordarrelle',\n",
       " 'breaking*',\n",
       " 'monologue',\n",
       " '15626',\n",
       " 'braves.',\n",
       " 'super7',\n",
       " 'xbox...then',\n",
       " 'reconnects',\n",
       " 'sophomore',\n",
       " 'shmenocide.',\n",
       " 'heterosexual.',\n",
       " 'gulliani',\n",
       " 'statefarm.',\n",
       " 'knaus.',\n",
       " 'duchene.',\n",
       " 'tcs',\n",
       " 'holoflag',\n",
       " 'obsessing',\n",
       " 'momz',\n",
       " 'wavelike',\n",
       " 'everybody.',\n",
       " 'rondae',\n",
       " 'turned*',\n",
       " 'hose.',\n",
       " 'leppa',\n",
       " 'hurricane',\n",
       " '*muslim*.',\n",
       " 'borowiecki',\n",
       " '00001011',\n",
       " 'cig',\n",
       " 'easterns',\n",
       " 'jason..',\n",
       " 'tastiera',\n",
       " 'infp.',\n",
       " 'mono-red',\n",
       " 'dui.',\n",
       " 'saints.',\n",
       " 'online...',\n",
       " 'ricola',\n",
       " 'luisteren.',\n",
       " '*child*',\n",
       " 'mencionan',\n",
       " 'connects...',\n",
       " 'outlandishly',\n",
       " 'tom_robinson',\n",
       " 'upskirts',\n",
       " 'lionhead',\n",
       " 'garrus.',\n",
       " 'jetter',\n",
       " 'non-cryptocurrency',\n",
       " 'fps...i',\n",
       " '*downvotes',\n",
       " 'swarfega',\n",
       " 'sensors',\n",
       " '5.8',\n",
       " '170mm.',\n",
       " 'molesters...',\n",
       " 'bouncy-bouncy',\n",
       " '^^^11am',\n",
       " 'scrivi',\n",
       " 'hahaha....belter',\n",
       " 'hardpoints',\n",
       " 'responder',\n",
       " 'dpt',\n",
       " 'backwards-compatible.',\n",
       " '*there*',\n",
       " 'noodley',\n",
       " 'war-mongers',\n",
       " 'nerubian',\n",
       " 'creeper',\n",
       " '*maps*',\n",
       " 'ugins',\n",
       " 'margins.',\n",
       " 'dev.i',\n",
       " 'that...thanks',\n",
       " 'mins',\n",
       " 'goer.',\n",
       " 'hiskon',\n",
       " 'signs.',\n",
       " 'chemical...',\n",
       " 'bradord',\n",
       " 'liberators.',\n",
       " '*still',\n",
       " 'rears',\n",
       " 'horen',\n",
       " 'battlecruiser',\n",
       " 'notifications...',\n",
       " 'paperboy',\n",
       " 'transbay',\n",
       " 'boie',\n",
       " '*one',\n",
       " 'stalked',\n",
       " 'hooligan',\n",
       " 'cim2',\n",
       " 'yo-ho-ho',\n",
       " '^players',\n",
       " 'chef.\\\\',\n",
       " 'antes',\n",
       " 'it...ban',\n",
       " 'out-vote',\n",
       " 'lolking',\n",
       " 'tastemakers',\n",
       " 'lady-boner',\n",
       " 'armed...',\n",
       " 'lennyface.jpg',\n",
       " '*smacker*',\n",
       " 'chinese-made',\n",
       " 'bbl',\n",
       " 'saying...but',\n",
       " 'heinlein',\n",
       " 'godchancellor',\n",
       " 'rigged.',\n",
       " 'guvmint',\n",
       " 'witche',\n",
       " '*smell',\n",
       " 'trade**',\n",
       " 'gyarados',\n",
       " 'naiv',\n",
       " 'chairforce',\n",
       " 'vamps',\n",
       " 'daaaaaaaamn',\n",
       " 'traum-',\n",
       " 'normie.',\n",
       " 'downtime...',\n",
       " 'thec',\n",
       " 'ppm',\n",
       " 'odesk',\n",
       " 'emotionally-',\n",
       " 'nolandbeyond',\n",
       " 'daybreak-downtime',\n",
       " 'maaaaaatloooock',\n",
       " 'unvaccinated',\n",
       " 'injury',\n",
       " 'sidearms.',\n",
       " 'hastie',\n",
       " 'phonetic',\n",
       " '-quit',\n",
       " 'shocker....',\n",
       " 'attys',\n",
       " '1-5..',\n",
       " 'wiper',\n",
       " 'cursed',\n",
       " 'aaaahhhhhhahahaa.',\n",
       " 'safe......but',\n",
       " 'spy...',\n",
       " 'ejamacaxtions',\n",
       " 'fortunes.',\n",
       " 'scratched',\n",
       " 'gifrecipes',\n",
       " '**clinically',\n",
       " 'donor',\n",
       " 'zakir',\n",
       " 'creditable.',\n",
       " 'stickied..',\n",
       " 'hobnobs',\n",
       " 'basques',\n",
       " 'o-line....',\n",
       " 'misnomer.',\n",
       " 'corresponding',\n",
       " 'tree-mangled',\n",
       " 'muangthong',\n",
       " 'imaginaaaation',\n",
       " 'enger',\n",
       " 'certs...',\n",
       " 'dankuwel',\n",
       " '320x200',\n",
       " 'escapist',\n",
       " 'keymod',\n",
       " '3000rpm',\n",
       " 'ballpark...',\n",
       " 'florsheim',\n",
       " 'fogeys',\n",
       " 'connerly',\n",
       " 'accessed',\n",
       " 'many............',\n",
       " 'trank',\n",
       " 'have...',\n",
       " 'not...',\n",
       " 'libtards...',\n",
       " '#**jeb',\n",
       " 'multi-level-marketing',\n",
       " '2101',\n",
       " 'goosebumps',\n",
       " 'struggle....',\n",
       " 'sombreros',\n",
       " 'paradises',\n",
       " 'autoinstall',\n",
       " 'surtout',\n",
       " 'rekting',\n",
       " 'washcloth',\n",
       " 'adaption.',\n",
       " 'avenue.',\n",
       " 'bio-broly',\n",
       " 'jota',\n",
       " 'citizenship.',\n",
       " 'dreamy.',\n",
       " 'archano-capitalist',\n",
       " '5-hole',\n",
       " 'shilly',\n",
       " 'npm',\n",
       " 'polnareff',\n",
       " 'chaox',\n",
       " 'said...',\n",
       " 'cylons.i',\n",
       " 'rol',\n",
       " 'reads.',\n",
       " '1401',\n",
       " 'euro-stepping',\n",
       " 'formulas.',\n",
       " 'sow.',\n",
       " 'politely.',\n",
       " 'raise.',\n",
       " 'showe',\n",
       " 'cause.',\n",
       " 'ramston',\n",
       " 'enforce',\n",
       " 'vina',\n",
       " 'raw_input',\n",
       " 'buttfeed',\n",
       " 'mission....for',\n",
       " 'planeswalker.',\n",
       " 'cars..',\n",
       " 'fe7',\n",
       " 'activeate',\n",
       " '5$',\n",
       " '#coalisgoodforhumanity',\n",
       " 'cj3k',\n",
       " 'twat-monkey',\n",
       " 'lmgs.',\n",
       " 'koa-789.',\n",
       " 'proctored',\n",
       " '50mph.',\n",
       " 'two-metre-peter',\n",
       " 'racists*.',\n",
       " 'modchecker',\n",
       " 'tedious.',\n",
       " 'salads',\n",
       " 'simulation',\n",
       " 'autobounce',\n",
       " '362d',\n",
       " '`whatever',\n",
       " 'nigh-unbeatable',\n",
       " 'okaay',\n",
       " 'ronal',\n",
       " 'intr-o',\n",
       " 'goonjar',\n",
       " 'duuuuuurr',\n",
       " 'xanadu.',\n",
       " 'favored',\n",
       " 'paranoidandroid',\n",
       " 'ronda.',\n",
       " 'summer-sun',\n",
       " 'underpants.',\n",
       " 'raaaaaaaaaaacist',\n",
       " '55-57',\n",
       " '*wanted*',\n",
       " 'sponges.',\n",
       " 'resembles',\n",
       " 'dudacated*',\n",
       " '*entitled',\n",
       " 'geometry.',\n",
       " 'counterargument',\n",
       " 'kapok',\n",
       " 'non-ingress',\n",
       " 'lifescare',\n",
       " 'kanojo',\n",
       " 'then...or',\n",
       " 'jaydenohara',\n",
       " 'catliyn',\n",
       " 'lelelele.',\n",
       " 'absurdum',\n",
       " '$3-4',\n",
       " 'characteristic',\n",
       " '389',\n",
       " 'mechwarper',\n",
       " 'commissioner',\n",
       " 'embarrassingly-misguided',\n",
       " 'mea',\n",
       " 'posey',\n",
       " 'out-putin',\n",
       " '#shh',\n",
       " 'break....',\n",
       " 'specifically.',\n",
       " 'glazing-what',\n",
       " 'clea',\n",
       " 'huhs',\n",
       " 'liya',\n",
       " 'dlcs',\n",
       " '*ethos*.',\n",
       " 'vvgt',\n",
       " 'finnland',\n",
       " 'saucer',\n",
       " 'dang....',\n",
       " 'schuldig',\n",
       " 'disfigure',\n",
       " '*horrible',\n",
       " 'raikonen',\n",
       " 'points^^tm.',\n",
       " 'fastlane',\n",
       " 'melted',\n",
       " '#rapgametaylorswift',\n",
       " 'vietcong',\n",
       " '12c',\n",
       " 'fok',\n",
       " 'payg',\n",
       " 'inscription',\n",
       " '38-1',\n",
       " 'non-premium',\n",
       " 'tone*',\n",
       " '100-level',\n",
       " 'sounds***better',\n",
       " 'leaver-buster',\n",
       " 'vociferous',\n",
       " 'pores.',\n",
       " 'liberace.',\n",
       " 'doona',\n",
       " 'hrs...',\n",
       " 'daily...wake',\n",
       " 'doolittle',\n",
       " 'ubigod',\n",
       " 'reblogs',\n",
       " 'piecesof',\n",
       " 'luchalitten',\n",
       " 'apples..',\n",
       " 'yeah...thats',\n",
       " 'vulturon',\n",
       " 'associations.',\n",
       " '*grinds',\n",
       " 'surpresa',\n",
       " 'misdemeanor.',\n",
       " 'cosleeping',\n",
       " 'silverstein.',\n",
       " 'field....why',\n",
       " 'gaygas',\n",
       " 'versaille',\n",
       " 'involv**e**d',\n",
       " 'windseeker',\n",
       " 'unexpectedly',\n",
       " 'tenders',\n",
       " 'onderdrukt',\n",
       " 'jamiro',\n",
       " 'pinagka-iba',\n",
       " '7-12v',\n",
       " 'theys',\n",
       " 'uzbekistan',\n",
       " 'e-50',\n",
       " 'trans-glutaminase',\n",
       " 'brotacular',\n",
       " 'fatflix',\n",
       " 'praten',\n",
       " '^^^me',\n",
       " 'individuality.',\n",
       " 'aziz.',\n",
       " 'hebleb',\n",
       " 'users...',\n",
       " 'puffs',\n",
       " '52k',\n",
       " 'darbar',\n",
       " '-benguet',\n",
       " 'pygmy',\n",
       " 'precession.',\n",
       " 'shogun2',\n",
       " 'chloe.',\n",
       " 'p229',\n",
       " 'over-estimation.',\n",
       " 'pixelart',\n",
       " 'ingredients...wait',\n",
       " 'lawled.',\n",
       " '^~~#notallmen~~',\n",
       " 'maryjane',\n",
       " 'edition*',\n",
       " 'signaler*',\n",
       " 'sacre',\n",
       " 'redeye',\n",
       " 'mis-leading',\n",
       " 'that...but',\n",
       " 'job~',\n",
       " '^^met',\n",
       " 'recommendation.',\n",
       " 'halftime',\n",
       " '~~november~~',\n",
       " 'pujie',\n",
       " 'ho-ohs',\n",
       " 'drawboard',\n",
       " 'hacs',\n",
       " 'lesser-liked',\n",
       " 'lol...need',\n",
       " 'aaaaaaaaaaaaarabic...',\n",
       " 'pansy.',\n",
       " 'strecthed',\n",
       " 'pitbull.',\n",
       " '9gag.',\n",
       " 'magnezone.',\n",
       " 'shinseki',\n",
       " '821',\n",
       " 'dipping.',\n",
       " 'ensouled',\n",
       " 'ripened',\n",
       " 'frendly',\n",
       " 'prophet',\n",
       " 'eyelashes.',\n",
       " 'chewy',\n",
       " 'being.',\n",
       " 'dielectric',\n",
       " 'scoobie',\n",
       " 'srb',\n",
       " 'bamboozle',\n",
       " '1933',\n",
       " 'fuckeyes',\n",
       " 'electronic.',\n",
       " 'see..',\n",
       " 'scathach',\n",
       " 'airplay',\n",
       " 'vederla',\n",
       " '-valley',\n",
       " 'well-showered...almost',\n",
       " 'scallion',\n",
       " 'informatics',\n",
       " 'sailboats.',\n",
       " 'ameristralia',\n",
       " 'so....kim',\n",
       " 'non-scientific',\n",
       " 'achilles.',\n",
       " 'hwtf',\n",
       " 'filbert',\n",
       " 'pocus...',\n",
       " 'prosecutions.',\n",
       " 'dastardly.',\n",
       " '88.....',\n",
       " 'yearling',\n",
       " 'commmunnicate',\n",
       " 'disturbance',\n",
       " '1011',\n",
       " 'xdeeee',\n",
       " 'zeff',\n",
       " 'nqwkejbfc',\n",
       " 'exaclty',\n",
       " 'leecher.',\n",
       " 'aaahhhhhh',\n",
       " 'guyssssss.',\n",
       " 'nacht.',\n",
       " 'mindset',\n",
       " 'tenets',\n",
       " 'waterbender',\n",
       " 'tng.',\n",
       " 'slasher',\n",
       " '*kinds*',\n",
       " 'abc',\n",
       " 'sthap',\n",
       " 'zavala',\n",
       " '~~kibler~~',\n",
       " 'unfit',\n",
       " 'neoliberal...',\n",
       " 'politico.',\n",
       " 'wallow.',\n",
       " 'gt-86',\n",
       " 'forgiven.',\n",
       " 'sayyyy',\n",
       " 'reitetsu',\n",
       " 'elsinore.',\n",
       " 'starla',\n",
       " 'amplifiers',\n",
       " '260x',\n",
       " 'tradin',\n",
       " 'corvettes',\n",
       " 'pointless...',\n",
       " 'yiddo',\n",
       " 'fx-8320e*',\n",
       " 'rond',\n",
       " 'corporate',\n",
       " '*secret',\n",
       " 'slant-eyed',\n",
       " 'same~',\n",
       " 'seus',\n",
       " 'deepcroft',\n",
       " 'lonnie',\n",
       " 'upside',\n",
       " 'lios',\n",
       " 'but....america',\n",
       " 'coloured',\n",
       " '~~nig~~',\n",
       " '*profit',\n",
       " 'delle',\n",
       " 'airbender*',\n",
       " 'podolski.',\n",
       " 'get-out-of-jail',\n",
       " 'inglorius',\n",
       " 'handcuffed',\n",
       " 'databases...',\n",
       " 'angryharpdarp',\n",
       " '1.8f',\n",
       " 'warna',\n",
       " 'ottw',\n",
       " 'moire',\n",
       " '120gb',\n",
       " 'pos.',\n",
       " 'conveniant',\n",
       " 'ragazze',\n",
       " '5b',\n",
       " 'kill...',\n",
       " '70.',\n",
       " 'hqm',\n",
       " '*oppressing',\n",
       " 'bendin',\n",
       " 'pissbabies',\n",
       " 'won*',\n",
       " 'millena',\n",
       " 'sawtooths',\n",
       " 'assafir',\n",
       " 'crimes...',\n",
       " 'corsica...',\n",
       " 'camer',\n",
       " 'reckful.',\n",
       " 'tallahassee',\n",
       " '***fixing***',\n",
       " 'advanced_',\n",
       " 'heterophobic',\n",
       " 'inherant',\n",
       " 'gehoord',\n",
       " '**she',\n",
       " 'fuck_me.jpg',\n",
       " 'arena.',\n",
       " 'muffler',\n",
       " 'inception',\n",
       " 'becoz',\n",
       " 'montie',\n",
       " 'dunnnnn',\n",
       " 'inizio',\n",
       " 'equivalent.',\n",
       " 'stiffness.',\n",
       " 'sugilite',\n",
       " 'peecs',\n",
       " 'strident...',\n",
       " 'kobains',\n",
       " 'unsee...',\n",
       " 'versta',\n",
       " 'niggerian',\n",
       " 'lordhern',\n",
       " 'stfu',\n",
       " 'so...cause',\n",
       " 'occazzo',\n",
       " 'prijavio',\n",
       " 'elba.....',\n",
       " 'fsggot',\n",
       " 'slumbering...',\n",
       " 'antibiotics',\n",
       " 'caragor',\n",
       " 'enticing',\n",
       " 'roadhead',\n",
       " 'joyous',\n",
       " 'ospreys',\n",
       " 'twitch..',\n",
       " 'cattlewomen',\n",
       " 'ne7',\n",
       " 'viata-i',\n",
       " 'knees...',\n",
       " 'strum.',\n",
       " 'perma-banned.',\n",
       " 'insemenations',\n",
       " 'wentz',\n",
       " 'guessed',\n",
       " 'barrowman',\n",
       " 'unterbewerteter',\n",
       " 'compooter',\n",
       " 'gallbladder',\n",
       " '.r3d',\n",
       " 'camero',\n",
       " 'curries.',\n",
       " 'sarcasm.png',\n",
       " 'mit',\n",
       " 'spinnin',\n",
       " 'situtation.',\n",
       " 'slangdumb-ass',\n",
       " 'playskins',\n",
       " 'overstepping',\n",
       " 'name.',\n",
       " 'inr',\n",
       " 'writing....',\n",
       " 'killer~~',\n",
       " 'horacio',\n",
       " 'water-pipe',\n",
       " 'relevantslavia',\n",
       " 'kilometers',\n",
       " 'nameplate',\n",
       " 'bristle',\n",
       " 'motoleaks',\n",
       " 'magnezone',\n",
       " 'gabey',\n",
       " 'ev',\n",
       " 'qeii',\n",
       " 'chloroform',\n",
       " 'manliest',\n",
       " '*exodus*',\n",
       " '*rape',\n",
       " 'ghee',\n",
       " '..isn',\n",
       " 'colorado.',\n",
       " 'kamag-anak',\n",
       " 'mormom',\n",
       " '0ss',\n",
       " 'achievement-hunter',\n",
       " 'debatable',\n",
       " 'fear*',\n",
       " 'inconspicuous',\n",
       " 'tf2outpost',\n",
       " 'enfants',\n",
       " '9-9-9',\n",
       " 'useable.',\n",
       " 'horcrux',\n",
       " 'invests',\n",
       " 'descriptive.',\n",
       " '#design_space',\n",
       " 'fm',\n",
       " 'superturismos',\n",
       " 'ep2...',\n",
       " 'voyboy',\n",
       " 'bondooooooooooo',\n",
       " 'in\\\\*',\n",
       " 'supervillain',\n",
       " '**wrong**',\n",
       " 'pixels...',\n",
       " 'dubs.',\n",
       " 'skifonix',\n",
       " 'reversed...',\n",
       " 'emalkay',\n",
       " 'scientists...',\n",
       " 'larceny',\n",
       " 'awning.',\n",
       " 'meisjes.',\n",
       " 'cliche...',\n",
       " 'e-leagueal',\n",
       " 're-offending',\n",
       " 'overeactions.',\n",
       " 'right..we',\n",
       " 'victimless',\n",
       " 'on....i',\n",
       " 'katanas...',\n",
       " 'week-out',\n",
       " 'watch_foxes',\n",
       " 'commonwealth.',\n",
       " '...username',\n",
       " 'slender',\n",
       " 'abolutes.',\n",
       " 'laifu.',\n",
       " 'relieved..',\n",
       " '*brainsplode*',\n",
       " 'minimum.',\n",
       " 'shrimati',\n",
       " 'padoin',\n",
       " 'hebe',\n",
       " 'no...statistically',\n",
       " 'particuliar',\n",
       " 'leveles',\n",
       " 'cinephile',\n",
       " 'lgd...',\n",
       " 'scatman',\n",
       " 'shear',\n",
       " 'well-taken',\n",
       " 'tune-up.',\n",
       " '*effort*',\n",
       " 'denounce',\n",
       " 'unenjoyable.',\n",
       " 'rypien',\n",
       " '**cal',\n",
       " '.........',\n",
       " 'post-disaster',\n",
       " 'poblano',\n",
       " 'intended........',\n",
       " 'tashfeen',\n",
       " 'jadzia',\n",
       " 'flouride',\n",
       " 'printen...',\n",
       " 'tsumugu',\n",
       " 'doosh',\n",
       " 'wondolowski',\n",
       " 'saaaaaaaaay',\n",
       " '*abuela*.',\n",
       " 'backlighting',\n",
       " 'bck',\n",
       " 'sans-ads',\n",
       " 'baidugo',\n",
       " 'endup',\n",
       " 'mutlu',\n",
       " 'headline...because',\n",
       " 'popoff',\n",
       " 'treasuries--the',\n",
       " 'side...arma',\n",
       " 'feinstein.',\n",
       " 'orbs.',\n",
       " 'rug...',\n",
       " '$1.29',\n",
       " 'haliburton',\n",
       " '----------------',\n",
       " 'sheathing',\n",
       " 'shawshank',\n",
       " 'digemon',\n",
       " 'dstwo',\n",
       " 'bill...',\n",
       " 'cramps...',\n",
       " 'besler',\n",
       " 'spoon...*',\n",
       " 'dvorak',\n",
       " 'nuk3town',\n",
       " 'nmsflashbacks',\n",
       " '**dawn',\n",
       " 'gladly.',\n",
       " '*asshole',\n",
       " 'mckinley',\n",
       " '*excuse',\n",
       " 'wigle',\n",
       " 'sadiq...',\n",
       " 'elderberries',\n",
       " '-bob',\n",
       " '$better',\n",
       " 'york.',\n",
       " 'industry...again.',\n",
       " 'marshall...',\n",
       " 'sticker**',\n",
       " 'essential',\n",
       " 'puffies',\n",
       " 'finegers',\n",
       " 'mcnopeface',\n",
       " 'ribbit',\n",
       " 'knowd',\n",
       " 'vraxx',\n",
       " 'drizzlin',\n",
       " 'uldaman.',\n",
       " 'tatela',\n",
       " 'taleos',\n",
       " 'acceptably',\n",
       " 'mordremoth',\n",
       " 'sparties',\n",
       " 'hand.',\n",
       " 'sweatdrop.jpg',\n",
       " 'osu-um',\n",
       " 'marijuanas',\n",
       " 'horror.',\n",
       " 'loan....',\n",
       " 'connor...',\n",
       " 'chinese-russian-communist',\n",
       " 'sitcom.',\n",
       " '~~fun~~',\n",
       " 'jusssst',\n",
       " '#freeedom',\n",
       " 'staggering.',\n",
       " 'tempered.',\n",
       " 'responses...',\n",
       " 'port',\n",
       " 'refraction',\n",
       " 'radbourn',\n",
       " 'relgione',\n",
       " 'stur',\n",
       " 'dangit',\n",
       " 'clasificados',\n",
       " 'tweeting',\n",
       " 'gobblegums',\n",
       " 'manoos',\n",
       " 'rickson',\n",
       " 'apostolate.',\n",
       " 'soundtrack',\n",
       " '1978',\n",
       " 'raping....',\n",
       " 'einer',\n",
       " 'readonly',\n",
       " 'hematite',\n",
       " '47.7',\n",
       " 'skin...........',\n",
       " 'disguise.......',\n",
       " 'diseases',\n",
       " 'ohmmeters',\n",
       " '1.69',\n",
       " 'grumpus',\n",
       " '#dumbasses',\n",
       " 'un-knowledgable',\n",
       " 'blocking.',\n",
       " 'drop...',\n",
       " 'perspectived',\n",
       " '840',\n",
       " 'oyes',\n",
       " 'debris.',\n",
       " '\\\\#it',\n",
       " 'has*',\n",
       " 'lamey',\n",
       " 'sleeker',\n",
       " 'graduated.',\n",
       " 'canted',\n",
       " 'maimutele.',\n",
       " 'morokko',\n",
       " 'happen***',\n",
       " 'fatphobia',\n",
       " 'alphabetize',\n",
       " 'phasma',\n",
       " 'cynade',\n",
       " 'poach',\n",
       " '300-style',\n",
       " 'quickscopez',\n",
       " 'fairly.',\n",
       " 'heee^',\n",
       " 'coulson.',\n",
       " '*shuns*',\n",
       " 'applicasion',\n",
       " 'merlings',\n",
       " '0.714',\n",
       " 'r33',\n",
       " 'baezemore',\n",
       " '*triggered*',\n",
       " 'pira',\n",
       " 'feedings',\n",
       " 'sir-child',\n",
       " 'proved',\n",
       " 'aronge',\n",
       " 'lolo',\n",
       " 'subcommittee',\n",
       " 'lotto',\n",
       " 'clements',\n",
       " 'migrants...',\n",
       " '^^^^^me',\n",
       " 'stossel',\n",
       " 'copacetic.',\n",
       " 'larose',\n",
       " 'turtleback.',\n",
       " 'assusta-me',\n",
       " 'reclearing',\n",
       " 'ravers',\n",
       " 'howto.',\n",
       " 'incestiual',\n",
       " '10001001',\n",
       " '8-month',\n",
       " 'narritive',\n",
       " 'boxcutter',\n",
       " 'blundell',\n",
       " 'photographed',\n",
       " 'fellbadman',\n",
       " 'doggin.',\n",
       " 'stick-on',\n",
       " 'amas',\n",
       " 'supremacists...',\n",
       " 'pinko',\n",
       " 'ovechkin-malkin-panarin',\n",
       " 'banlist',\n",
       " 'macabre',\n",
       " '6-year',\n",
       " 'in-party',\n",
       " 'rattlesnk',\n",
       " '*sponsored',\n",
       " 'food-only',\n",
       " 'glaucoma',\n",
       " 'butchiness',\n",
       " '*northern',\n",
       " '4.0.',\n",
       " 'subreddits',\n",
       " 'rages',\n",
       " 'upsells',\n",
       " 'tryyyy',\n",
       " 'searcher.',\n",
       " 'manitoba...',\n",
       " '169',\n",
       " 'brewmasters.',\n",
       " 'xolos',\n",
       " 'dominating',\n",
       " 'but',\n",
       " 'mga',\n",
       " '*edit*',\n",
       " 'naw...they',\n",
       " 'nightmares',\n",
       " '$1.43',\n",
       " 'plesioth',\n",
       " 'loan-word',\n",
       " 'toppest',\n",
       " 'saavedra',\n",
       " 'extremism.',\n",
       " 'normans',\n",
       " '6970',\n",
       " '1.22',\n",
       " 'bastante',\n",
       " 'realities',\n",
       " 'releived',\n",
       " 'ashby',\n",
       " '71.11',\n",
       " 'celerons',\n",
       " 'idkfa',\n",
       " 'tommed',\n",
       " '2028',\n",
       " 'law-abiding.',\n",
       " 'skips',\n",
       " 'democracy.',\n",
       " 'dardan',\n",
       " '2cents.',\n",
       " 'skin...though',\n",
       " 'tightens',\n",
       " 'parlais',\n",
       " 'verdere',\n",
       " 'bukkakke',\n",
       " 'weeabo',\n",
       " '4400',\n",
       " 'ethnicitys.',\n",
       " 'abso-fucking-lutely',\n",
       " '49.49',\n",
       " 'pitted',\n",
       " 'luggage...wait',\n",
       " 'find...',\n",
       " 'lanceros',\n",
       " 'lemmy*',\n",
       " 'slothfulness.',\n",
       " 'easyway',\n",
       " 'appluding',\n",
       " 'sown',\n",
       " '*powderly',\n",
       " 'exaggerate',\n",
       " 'africa....then',\n",
       " 'bare-knuckle',\n",
       " 'loved',\n",
       " 'passing....',\n",
       " 'spending..',\n",
       " 'pressurized',\n",
       " 'barrage**.',\n",
       " 'watamote',\n",
       " 'sucking',\n",
       " 'kuchh',\n",
       " 'tarja',\n",
       " '.class',\n",
       " 'ctcoishosn',\n",
       " 'internet-dumbass',\n",
       " 'unachievable',\n",
       " 'shilling',\n",
       " 'fatalis',\n",
       " '...posting',\n",
       " 'entendres.',\n",
       " 'dls',\n",
       " 'lint-free',\n",
       " 'ings',\n",
       " 'yorkmark',\n",
       " '...maggot',\n",
       " '4-br',\n",
       " 'comy',\n",
       " 'don`t',\n",
       " 'wretched',\n",
       " 'truuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu',\n",
       " 'lawful',\n",
       " 'stickybomb',\n",
       " 'muhahahaa',\n",
       " 'post-debate',\n",
       " 'reprezentativ',\n",
       " 'steven.',\n",
       " '#ted',\n",
       " 'likeness',\n",
       " 'dioner',\n",
       " 'allergies',\n",
       " '~5-10',\n",
       " '**darkness**',\n",
       " 'benzes',\n",
       " 'argument',\n",
       " 'duolingo',\n",
       " 'clar',\n",
       " '...daddy',\n",
       " 'know...people',\n",
       " 'coop.',\n",
       " 'broadcasts',\n",
       " 'liar.',\n",
       " 'outdmg',\n",
       " 'copenhagen',\n",
       " 'eclipse.',\n",
       " 'fuddy-duddy',\n",
       " 'huroof.',\n",
       " 'month-long',\n",
       " 'poulan',\n",
       " 'dickface',\n",
       " 'capodanno',\n",
       " '\\\\desktop\\\\smartas~\\\\conspira~\\\\flatear~.wk3',\n",
       " 'kilff',\n",
       " 'codcompetitive',\n",
       " 'mode',\n",
       " '####lestemmasterrace',\n",
       " 'marantz.',\n",
       " 'gloopity',\n",
       " 'economics.',\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdb6c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gensim.downloader as api\n",
    "model = api.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2a1d07a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('kings', 0.7138046622276306), ('queen', 0.6510956287384033), ('monarch', 0.6413194537162781), ('crown_prince', 0.6204220056533813), ('prince', 0.6159993410110474), ('sultan', 0.5864824056625366), ('ruler', 0.5797567367553711), ('princes', 0.5646552443504333), ('Prince_Paras', 0.543294370174408), ('throne', 0.5422105193138123)]\n",
      "[('queen', 0.7118191719055176), ('monarch', 0.6189674735069275), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321839332581), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.5181134343147278), ('sultan', 0.5098593831062317), ('monarchy', 0.5087412595748901)]\n",
      "[ 3.59375000e-01  4.15039062e-02  9.03320312e-02  5.46875000e-02\n",
      " -1.47460938e-01  4.76074219e-02 -8.49609375e-02 -2.04101562e-01\n",
      "  3.10546875e-01 -1.05590820e-02 -6.15234375e-02 -1.55273438e-01\n",
      " -1.52343750e-01  8.54492188e-02 -2.70996094e-02  3.84765625e-01\n",
      "  4.78515625e-02  2.58789062e-02  4.49218750e-02 -2.79296875e-01\n",
      "  9.09423828e-03  4.08203125e-01  2.40234375e-01 -3.06640625e-01\n",
      " -1.80664062e-01  4.73632812e-02 -2.63671875e-01  9.08203125e-02\n",
      "  1.37695312e-01 -7.20977783e-04  2.67333984e-02  1.92382812e-01\n",
      " -2.29492188e-02  9.70458984e-03 -7.37304688e-02  4.29687500e-01\n",
      " -7.93457031e-03  1.06445312e-01  2.80761719e-02 -2.29492188e-01\n",
      " -1.91650391e-02 -2.36816406e-02  3.51562500e-02  1.71875000e-01\n",
      " -1.12304688e-01  6.25000000e-02 -1.69921875e-01  1.29882812e-01\n",
      " -1.54296875e-01  1.58203125e-01 -7.76367188e-02  1.78710938e-01\n",
      " -1.72851562e-01  9.96093750e-02  3.94531250e-01  6.44531250e-02\n",
      " -6.83593750e-02 -3.18359375e-01  5.95703125e-02 -1.02539062e-02\n",
      "  9.37500000e-02  8.25195312e-02 -2.52685547e-02  1.09863281e-01\n",
      " -6.73828125e-02 -1.70898438e-01  6.78710938e-02  1.04492188e-01\n",
      " -2.11914062e-01  1.30859375e-01 -1.24573708e-05  1.85546875e-02\n",
      " -1.61132812e-01 -8.00781250e-02  9.42382812e-02 -8.78906250e-02\n",
      "  1.82617188e-01 -2.48718262e-03  8.74023438e-02  1.75781250e-01\n",
      " -2.17285156e-02 -1.96289062e-01  9.52148438e-02 -5.15136719e-02\n",
      "  1.01928711e-02  6.22558594e-02 -2.13867188e-01  2.25585938e-01\n",
      "  2.46093750e-01 -6.12792969e-02  1.74560547e-02 -1.46484375e-01\n",
      "  3.93676758e-03 -1.62109375e-01 -1.10839844e-01  6.88476562e-02\n",
      " -1.83593750e-01  1.13281250e-01  9.08203125e-02 -1.64062500e-01\n",
      " -3.71093750e-01 -5.39550781e-02 -8.66699219e-03 -1.18164062e-01\n",
      " -5.93261719e-02  8.74023438e-02 -1.98242188e-01 -1.36718750e-01\n",
      "  6.39648438e-02 -1.88476562e-01 -2.96875000e-01  6.39648438e-02\n",
      "  2.16796875e-01 -7.71484375e-02  1.13769531e-01  1.96533203e-02\n",
      "  2.31445312e-01  6.59179688e-02  1.02539062e-01 -6.39648438e-02\n",
      " -1.48437500e-01 -5.59082031e-02 -2.43164062e-01  2.71484375e-01\n",
      "  1.83593750e-01  3.06396484e-02 -2.01416016e-02 -1.53320312e-01\n",
      "  7.08007812e-02 -2.35595703e-02 -9.66796875e-02 -2.83203125e-01\n",
      " -2.57568359e-02 -7.42187500e-02 -4.27246094e-02  6.98242188e-02\n",
      " -1.74804688e-01  2.27539062e-01  2.92968750e-01 -1.86767578e-02\n",
      "  2.94921875e-01 -1.12304688e-01  4.85839844e-02 -2.15820312e-01\n",
      "  1.03149414e-02 -1.14257812e-01 -6.39648438e-02  7.27539062e-02\n",
      " -1.47460938e-01 -2.16796875e-01  1.32812500e-01  1.83593750e-01\n",
      " -1.48437500e-01 -1.31835938e-01 -3.73535156e-02  1.19628906e-01\n",
      " -2.01171875e-01  1.00097656e-01 -8.93554688e-02  1.23596191e-03\n",
      "  7.17773438e-02  1.42578125e-01 -3.03955078e-02 -1.89453125e-01\n",
      " -8.88671875e-02  3.83300781e-02 -1.74804688e-01 -3.66210938e-03\n",
      " -2.08007812e-01  8.97216797e-03  2.35351562e-01  1.06933594e-01\n",
      " -2.65625000e-01 -2.16796875e-01  7.08007812e-02  9.08203125e-02\n",
      "  3.00781250e-01 -1.07421875e-01  1.01562500e-01 -6.25000000e-02\n",
      "  1.33789062e-01 -1.62353516e-02  2.50000000e-01 -1.72851562e-01\n",
      "  3.32031250e-01  1.12304688e-01 -1.47705078e-02 -1.04980469e-01\n",
      " -8.05664062e-02  3.30078125e-01  9.32617188e-02 -1.47460938e-01\n",
      " -2.05078125e-01 -7.56835938e-02 -1.04492188e-01  6.25000000e-02\n",
      " -2.02148438e-01 -1.09375000e-01 -8.05664062e-02  5.49316406e-02\n",
      " -8.88671875e-02  5.24902344e-02 -2.23632812e-01  5.17578125e-02\n",
      " -1.83593750e-01 -6.73828125e-02 -9.13085938e-02  1.29882812e-01\n",
      " -2.31933594e-02 -1.04003906e-01  1.79687500e-01  8.34960938e-02\n",
      " -8.78906250e-02 -2.17773438e-01 -6.34765625e-02  1.33789062e-01\n",
      "  1.62109375e-01  2.87109375e-01 -1.14257812e-01  6.05468750e-02\n",
      "  1.49414062e-01 -3.08227539e-03  1.96289062e-01 -8.98437500e-02\n",
      "  1.45507812e-01  1.02539062e-02  1.22070312e-02  3.20312500e-01\n",
      "  1.24511719e-01  1.20849609e-02 -1.78710938e-01  3.71093750e-02\n",
      "  6.98242188e-02  1.62109375e-01  9.86328125e-02 -2.61718750e-01\n",
      "  1.89453125e-01 -2.83203125e-02  4.06250000e-01  3.56445312e-02\n",
      "  3.10058594e-02  2.27050781e-02  1.30859375e-01 -1.05957031e-01\n",
      "  8.69140625e-02 -9.76562500e-02  1.89453125e-01  3.17382812e-02\n",
      "  1.10351562e-01  2.11914062e-01 -1.66992188e-01  1.45263672e-02\n",
      "  1.15234375e-01  1.59179688e-01  9.91210938e-02 -2.40234375e-01\n",
      " -2.34375000e-01  1.74804688e-01  1.20605469e-01 -3.67187500e-01\n",
      " -7.81250000e-02  1.10839844e-01 -3.35937500e-01 -9.81445312e-02\n",
      " -7.47070312e-02 -1.89453125e-01  7.81250000e-02 -2.53906250e-01\n",
      " -6.03027344e-02 -2.46093750e-01 -9.37500000e-02  8.64257812e-02\n",
      "  1.15722656e-01 -1.24511719e-01  1.61132812e-01 -6.03027344e-02\n",
      " -2.47070312e-01 -9.52148438e-02 -4.05273438e-02  2.51953125e-01\n",
      " -1.95312500e-01 -1.31835938e-01  6.88476562e-02  2.67333984e-02\n",
      "  1.03027344e-01  1.05957031e-01 -3.01513672e-02  3.04687500e-01\n",
      " -8.74023438e-02  1.19140625e-01 -1.74560547e-02  8.78906250e-03\n",
      " -1.38671875e-01 -2.85156250e-01  2.29492188e-01 -3.55468750e-01\n",
      "  9.52148438e-03 -4.07714844e-02 -8.88671875e-02 -1.39160156e-02]\n"
     ]
    }
   ],
   "source": [
    "# Process the 'comment' column in your custom DataFrame\n",
    "comments = df['comment'].tolist()\n",
    "\n",
    "# Example: Find similar words to a given word\n",
    "similar_words = model.most_similar('king')\n",
    "print(similar_words)\n",
    "\n",
    "# Example: Perform vector arithmetic\n",
    "result = model.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "print(result)\n",
    "\n",
    "# Example: Access the vector representation of a word\n",
    "vector = model['word']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d435f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "####unique_words = set(df['comment'].str.split().sum())  # Assuming 'comment' is the column containing the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9363b1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing words: 100%|██████████████| 271808/271808 [00:02<00:00, 92182.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV words: 206852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "oov_words = []\n",
    "\n",
    "# Iterate over each word in unique_words and check if it is not present in the model\n",
    "for word in tqdm(unique_words, desc='Processing words'):\n",
    "    if word not in model:\n",
    "        oov_words.append(word)\n",
    "\n",
    "# Display the total number of OOV words\n",
    "print(\"Total OOV words:\", len(oov_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "078580f9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'nc and nh.' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#vectorise the comments\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m comment_vectors \u001b[38;5;241m=\u001b[39m [model[comment_word] \u001b[38;5;28;01mfor\u001b[39;00m comment_word \u001b[38;5;129;01min\u001b[39;00m comments]\n",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#vectorise the comments\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m comment_vectors \u001b[38;5;241m=\u001b[39m [\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcomment_word\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m comment_word \u001b[38;5;129;01min\u001b[39;00m comments]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'nc and nh.' not present\""
     ]
    }
   ],
   "source": [
    "#vectorise the comments\n",
    "###comment_vectors = [model[comment_word] for comment_word in comments]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ecaa152",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########import numpy as np\n",
    "\n",
    "# Determine the vocab_size and embedding_dim\n",
    "vocab_size = (TOTAL_WORDS + 1)  # Add 1 for the OOV token\n",
    "embedding_dim = 300  # Example dimensionality, replace with your desired value\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_matrix = np.random.rand(vocab_size, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5634fb6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(oov_words):\n\u001b[0;32m----> 2\u001b[0m     embedding_matrix[\u001b[43mword_index\u001b[49m[word]] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(embedding_dim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_index' is not defined"
     ]
    }
   ],
   "source": [
    "#############for i, word in enumerate(oov_words):\n",
    "    embedding_matrix[word_index[word]] = np.random.rand(embedding_dim + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e06b93e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 166508\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=TOTAL_WORDS)\n",
    "tokenizer.fit_on_texts(list(df['comment']))\n",
    "\n",
    "# Create the word_index dictionary\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Print the number of unique words in the tokenizer\n",
    "vocab_size = len(word_index)\n",
    "print(\"Vocabulary size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "233d7845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_matrix = np.random.rand(vocab_size + 1, embedding_dim)\n",
    "\n",
    "# Initialize the OOV word vectors\n",
    "for word, i in word_index.items():\n",
    "    if word in oov_words:\n",
    "        embedding_matrix[i] = np.random.rand(embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc92c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = MAX_LEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91f745b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define the model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m----> 6\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Embedding(vocab_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, embedding_dim, weights\u001b[38;5;241m=\u001b[39m[embedding_matrix], input_length\u001b[38;5;241m=\u001b[39m\u001b[43mmax_length\u001b[49m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LSTM(\u001b[38;5;241m128\u001b[39m))\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_length' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size + 1, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_data, target, batch_size=32, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f644f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Define the model architecture and set the embedding layer with the embedding matrix\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size + 1, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8631901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define the number of epochs\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Train the model with tqdm progress bar\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # Shuffle the training data\n",
    "    indices = np.arange(len(train_data))\n",
    "    np.random.shuffle(indices)\n",
    "    shuffled_train_data = train_data[indices]\n",
    "    shuffled_target = target.iloc[indices]  # Corrected indexing for target\n",
    "    \n",
    "    # Iterate over mini-batches and update the model\n",
    "    for i in range(0, len(train_data), batch_size):\n",
    "        batch_train_data = shuffled_train_data[i:i+batch_size]\n",
    "        batch_target = shuffled_target[i:i+batch_size]\n",
    "        \n",
    "        # Train on the batch\n",
    "        model.train_on_batch(batch_train_data, batch_target)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(train_data, target)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e7e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
